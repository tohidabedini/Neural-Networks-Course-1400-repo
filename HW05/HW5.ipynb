{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StudentName__HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "Nd4GU-Pf4vd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from gym import Env\n",
        "import datetime\n",
        "\n",
        "class FrozenLake(Env):\n",
        "    def __init__(self,studentNum:int=256, nonStationary = False):\n",
        "        self.studentNum = studentNum\n",
        "        self.nonStationary = nonStationary\n",
        "        \n",
        "        np.random.seed(self.studentNum)\n",
        "        self.beginMap = make_map(self.studentNum) #*2\n",
        "        self.beginMap[self.beginMap>1] = 1\n",
        "        self.endMap = make_map(self.studentNum + 100)\n",
        "        \n",
        "        self.changeDir = self.endMap - self.beginMap\n",
        "        self.changeDir *= 1/11000\n",
        "\n",
        "        self.fixedMap = self.beginMap\n",
        "\n",
        "        np.random.seed(datetime.datetime.now().microsecond)\n",
        "        \n",
        "        self.map = copy.deepcopy(self.fixedMap)\n",
        "        self.time = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.NSreset()\n",
        "        if not self.nonStationary:\n",
        "            self.map = copy.deepcopy(self.fixedMap)\n",
        "            self.time = 0\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def NSreset(self):\n",
        "        self.time += 1\n",
        "        self.map += self.changeDir\n",
        "\n",
        "        self.map[self.map>0.95]=0.95\n",
        "        self.map[self.map<0.0]=0.0\n",
        "\n",
        "        self.state = (0,0)\n",
        "        self.done = False\n",
        "        return self.state\n",
        "    \n",
        "    def states_transitions(self, state, action):\n",
        "        x = state[0]\n",
        "        y = state[1]\n",
        "        states = np.array([[x,y-1], [x,y+1], [x-1 ,y], [x+1,y] ])\n",
        "\n",
        "\n",
        "        if action == UP:\n",
        "            selected = states[2]\n",
        "        if action == DOWN:\n",
        "            selected = states[3]\n",
        "        if action == RIGHT:\n",
        "            selected = states[1]\n",
        "        if action == LEFT:\n",
        "            selected = states[0]\n",
        "\n",
        "        zero = np.zeros((4,2)).astype(int)\n",
        "        three = (3 * np.ones((4,2))).astype(int)\n",
        "        output = np.maximum(np.minimum(states, three),zero)\n",
        "        output, indices = np.unique(output, axis = 0, return_counts= True)\n",
        "\n",
        "        \n",
        "        selected = np.maximum(np.minimum(selected, three[0]), zero[0])\n",
        "        probs = indices * 0.025\n",
        "        probs[np.argmax(np.sum(selected == output, axis = 1))] += 0.9\n",
        "\n",
        "        return list(zip(output[:,0],output[:,1])), probs\n",
        "    \n",
        "    def possible_consequences(self,action:int,state_now=None):\n",
        "\n",
        "        if state_now==None:\n",
        "            state_now = self.state\n",
        "\n",
        "        state = [state_now[0],state_now[1]]\n",
        "        states, probs = self.states_transitions(state, action)\n",
        "        aa = np.array(states) \n",
        "        fail_probs = self.map[(aa[:,0]),(aa[:,1])]\n",
        "        dones = np.sum(aa == 3, axis = 1) == 2\n",
        "        return states, probs, fail_probs,dones\n",
        "    \n",
        "    def step(self, a:int):\n",
        "        if not (a in range(4)):\n",
        "            raise Exception(\"action is not available!!!\")\n",
        "        \n",
        "        states, probs, fail_probs,dones = self.possible_consequences(a)\n",
        "        \n",
        "        next_idx = np.random.choice(np.arange(len(states)), p = probs)\n",
        "        next_state = states[next_idx]\n",
        "        self.state = tuple(next_state)\n",
        "        \n",
        "        self.done = dones[next_idx]\n",
        "\n",
        "        r = -1\n",
        "\n",
        "        if self.done:\n",
        "            r += 60\n",
        "        elif np.random.rand()< fail_probs[next_idx]:\n",
        "            r -= 15\n",
        "            self.done = True\n",
        "\n",
        "        return (self.state, r, self.done, {})\n",
        "\n",
        "    def render(self,state=None):\n",
        "        if state == None:\n",
        "            state = self.state\n",
        "\n",
        "        out = \"\"\n",
        "        for i in range(4):\n",
        "            out += \"\\n------------------------------\\n| \"\n",
        "            for j in range(4):\n",
        "                if (i,j) == state:\n",
        "                    out += \"\\033[44m{:.3f}\\033[0m | \".format(self.map[i,j])\n",
        "                else :\n",
        "                    out += \"{:.3f} | \".format(self.map[i,j])\n",
        "\n",
        "        out += \"\\n------------------------------\"\n",
        "        print(out)\n",
        "\n",
        "    def environment_states(self):\n",
        "        env_states = []\n",
        "        for state_index in range(16):\n",
        "            s0 = state_index % 4\n",
        "            s1 = state_index//4\n",
        "            env_states.append((s0,s1))\n",
        "        return env_states\n",
        "\n",
        "        \n",
        "def set_max_min(var,maximum,minimum):\n",
        "    return min(max(var,minimum),maximum)\n",
        "\n",
        "def make_map(studentNum):\n",
        "    np.random.seed(studentNum)  \n",
        "    move = np.zeros(6)\n",
        "    idx = np.random.choice(range(6),size=3,replace=False)\n",
        "    move[idx] = 1\n",
        "\n",
        "    point = [0,0]\n",
        "    lowprobs = [tuple(point)]\n",
        "\n",
        "    for m in move:\n",
        "        if m:\n",
        "            point[0] += 1\n",
        "        else:\n",
        "            point[1] += 1\n",
        "        lowprobs.append(tuple(point))\n",
        "    \n",
        "    map = np.random.rand(4,4)\n",
        "    idx = np.array(lowprobs)\n",
        "\n",
        "    map[idx[:,0],idx[:,1]] = 0.001 \n",
        "    map[0,0] = 0.0\n",
        "    map[3,3] = 0.0 \n",
        "\n",
        "    return map"
      ],
      "metadata": {
        "id": "b8Ie-mkhiGjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Student ID"
      ],
      "metadata": {
        "id": "JIrM2PNQ4l5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STUDENT_NUM = #Student_ID"
      ],
      "metadata": {
        "id": "JvWeNrBx4or9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParameters"
      ],
      "metadata": {
        "id": "ba-NtxEn5LJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% allowed actions\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "\n",
        "ACTIONS = [LEFT,DOWN,RIGHT,UP]\n",
        "\n",
        "#%% hyperparameters\n",
        "EPISODES = 10000\n",
        "EPSILON = 0.1\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.9"
      ],
      "metadata": {
        "id": "54H4VswF5Kot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map of environment"
      ],
      "metadata": {
        "id": "Mdub8jub5bM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME5gllo7g0p7"
      },
      "outputs": [],
      "source": [
        "environment = FrozenLake(studentNum=STUDENT_NUM)\n",
        "\n",
        "print(\"Environment with fail probabilities :\")\n",
        "environment.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2><font color=indigo> Agent Implementation\n",
        "Implement your q-learning (off-policy TD) agent here. You need to utilize the step function provided in the Environment class to interact with frozen lake environment."
      ],
      "metadata": {
        "id": "9z-KEpaeOcAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "class Q_Learning:\n",
        "    def __init__(self, id, environment, discount , learning_rate = 0.1 , epsilon = 0.1 ,episodes=10000):\n",
        "\n",
        "        self.environment = environment\n",
        "        self.discount = discount\n",
        "        self.episodes = episodes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.environment = environment\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = 4\n",
        "\n",
        "\n",
        "######## Your Code Here ########"
      ],
      "metadata": {
        "id": "9P-5IZqIeco8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2><font color=indigo> Q Values\n",
        "Return the Q values that your agent learns in here:"
      ],
      "metadata": {
        "id": "c6J4uXQGuTgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Q_Learning('Your name', environment, 0.9 , learning_rate = 0.5 , epsilon = 0.1 ,episodes= 10000)\n",
        "Q , policy = ###"
      ],
      "metadata": {
        "id": "WYZfiWY6qMch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2><font color=darkcyan> Policy\n",
        "Return the optimal policy that your agent learns in here:"
      ],
      "metadata": {
        "id": "MMojYRGVvAXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy"
      ],
      "metadata": {
        "id": "9EFY3T0r9OHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}