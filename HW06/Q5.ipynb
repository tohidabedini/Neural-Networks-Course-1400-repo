{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddedfef3",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_defuzzify.html\n",
    "\n",
    "https://demonstrations.wolfram.com/TriangleAreaBisectors/\n",
    "\n",
    "https://stackoverflow.com/questions/19141432/python-numpy-machine-epsilon\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "https://www.geeksforgeeks.org/data-normalization-with-pandas/\n",
    "\n",
    "https://neelshelar.com/explain-anfis-architecture-with-a-neat-diagram/\n",
    "\n",
    "https://www.ques10.com/p/13355/explain-anfis-architecture-with-neat-diagram-1/\n",
    "\n",
    "https://www.tandfonline.com/doi/pdf/10.1623/hysj.51.4.588\n",
    "\n",
    "https://github.com/VManuelSM/Membership-functions/blob/master/membershipFunctions.py\n",
    "\n",
    "https://functionbay.com/documentation/onlinehelp/default.htm#!Documents/fuzzymembershipfunctions.htm\n",
    "\n",
    "https://github.com/gabrielegilardi/ANFIS\n",
    "\n",
    "https://pyswarms.readthedocs.io/en/latest/examples/usecases/train_neural_network.html\n",
    "\n",
    "https://towardsdatascience.com/how-to-split-a-dataframe-into-train-and-test-set-with-python-eaa1630ca7b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7265bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyswarms\n",
    "# !pip install scikit-fuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d4c6e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2aa55def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyswarms as ps\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d3260",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21deed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder for data in classification task and extract all classes\n",
    "def build_class_matrix(y):\n",
    "    \"\"\"\n",
    "    Builds the output array <y_out> for a classification problem. Array <y> has\n",
    "    dimensions (n_samples, 1) and <y_out> has dimension (n_samples, n_classes).\n",
    "    y_out[i,j] = 1 specifies that the i-th sample belongs to the j-th class.\n",
    "    \"\"\"\n",
    "    n_samples = y.shape[0]\n",
    "\n",
    "    # Classes and corresponding number\n",
    "    y_u, idx = np.unique(y, return_inverse=True)\n",
    "    n_classes = len(y_u)\n",
    "\n",
    "    # Build the array actually used for classification\n",
    "    y_out = np.zeros((n_samples, n_classes))\n",
    "    y_out[np.arange(n_samples), idx] = 1.0\n",
    "\n",
    "    return y_out, y_u\n",
    "\n",
    "\n",
    "# Sigmoid activation - not used in the code (it is commented - but if we perform a sigmoid activation on result of regreesion tasks, metrics will be improved. But it is because of random p_q_r parameters and network will perform well if it is optimized.)\n",
    "def sigmoid(x):\n",
    "    eps = np.finfo(np.float32).eps\n",
    "    if x>0:\n",
    "        return 1 / (1+ np.exp(-x) + eps)\n",
    "    else:\n",
    "        return 1 / (1+ np.exp(-x) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce561856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANFIS:\n",
    "    def __init__(self, network_IO_size, fuzzy_terms_per_feature, mf_mode):\n",
    "        self.network_IO_size = network_IO_size\n",
    "        self.fuzzy_terms_per_feature = fuzzy_terms_per_feature\n",
    "        self.mf_mode = mf_mode\n",
    "        self.mf_parameters = None\n",
    "        self.p_q_r_values = None\n",
    "        \n",
    "    \n",
    "    # triangle membership function\n",
    "    def triangle_function(self, params, x):\n",
    "        center = params[0]\n",
    "        radius = params[1]\n",
    "        abc = [center-radius, center, center+radius]\n",
    "        # other membership functions can be used too! -> look at here too know more about them: https://pythonhosted.org/scikit-fuzzy/api/skfuzzy.membership.html\n",
    "        value = fuzz.trimf(np.array([x]), abc)[0]\n",
    "        return value\n",
    "    \n",
    "    \n",
    "    # gbell membership function is preferred here because of easier random initialization (without constraint on order and relative magnitude of inputs).\n",
    "    def gbell_function(self, params, x):\n",
    "        a, b, c = params[0], params[1], params[2]\n",
    "        value = fuzz.membership.gbellmf(np.array([x]), a, b, c)[0]\n",
    "        return value\n",
    "    \n",
    "    # select membership function by the mf_mode parameter\n",
    "    def mf_function(self, params, x):\n",
    "        if self.mf_mode == \"tri\":\n",
    "            return self.triangle_function(params, x)\n",
    "        elif self.mf_mode == \"gbell\":\n",
    "            return self.gbell_function(params, x)\n",
    "        \n",
    "    \n",
    "\n",
    "    # generate membership values for all variables    \n",
    "    def mf_generators(self, input_data):\n",
    "        first_layer_out = np.zeros((self.fuzzy_terms_per_feature,self.network_IO_size[0]))\n",
    "        \n",
    "        fuzzy_terms_per_feature = self.fuzzy_terms_per_feature\n",
    "        \n",
    "\n",
    "        if fuzzy_terms_per_feature<=0:\n",
    "            print(\"Invalid fuzzy term number (1 or more! is required!).\")\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "            for i in range(fuzzy_terms_per_feature):\n",
    "                for j in range(len(input_data)):\n",
    "                    first_layer_out[i][j] = self.mf_function(self.mf_parameters[i][j], input_data[j])\n",
    "                \n",
    "        return first_layer_out\n",
    "\n",
    "    \n",
    "    # 2nd layer of ANFIS: dot product on mu values\n",
    "    def second_layer(self, first_layer_out):\n",
    "        second_layer_out = np.zeros(self.fuzzy_terms_per_feature)\n",
    "        for i in range(self.fuzzy_terms_per_feature):\n",
    "            second_layer_out[i] = np.prod(first_layer_out[i])\n",
    "            \n",
    "        return second_layer_out\n",
    "          \n",
    "        \n",
    "    # 3rd layer of ANFIS: normalizing previous layer values\n",
    "    def third_layer(self, second_layer_out):\n",
    "        third_layer_out = np.zeros(len(second_layer_out))\n",
    "        \n",
    "        if not(second_layer_out.any()):\n",
    "            third_layer_out = [0.5 , 0.5]\n",
    "        else:\n",
    "            for i in range(len(third_layer_out)):\n",
    "                third_layer_out[i] = (second_layer_out[i])/np.sum(second_layer_out)\n",
    "            \n",
    "        return third_layer_out\n",
    "    \n",
    "    \n",
    "    # apply arithmetic operations on parameters of defuzzification layer\n",
    "    def p_q_r_defuzzifier(self, input_data, p_q_r):\n",
    "        return np.sum(input_data*p_q_r[:-1]) + p_q_r[-1]\n",
    "            \n",
    "        \n",
    "    # 4th layer of ANFIS: defuzzification layer with p_q_r parameters\n",
    "    def fourth_layer(self, input_data, third_layer_out):\n",
    "        \n",
    "        fourth_layer_out = np.zeros((self.network_IO_size[1], len(third_layer_out)))\n",
    "        for i in range(self.network_IO_size[1]):\n",
    "            for j in range(len(fourth_layer_out[0])):\n",
    "                defuzzified_value = third_layer_out[j] * self.p_q_r_defuzzifier(input_data, self.p_q_r_values[i][j])\n",
    "                fourth_layer_out[i][j] = defuzzified_value\n",
    "            \n",
    "        return fourth_layer_out\n",
    "\n",
    "\n",
    "    # 5th layer of ANFIS: sum on previous layer outputs\n",
    "    def fifth_layer(self, fourth_layer_out):\n",
    "        fifth_layer_out = np.sum(fourth_layer_out,axis=-1)\n",
    "        return fifth_layer_out\n",
    "        \n",
    "    \n",
    "    # Sigmoid activation\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Numerically stable version of the sigmoid function (reference:\n",
    "        http://fa.bianp.net/blog/2019/evaluate_logistic/#sec3.)\n",
    "        \"\"\"\n",
    "        a = np.zeros_like(z)\n",
    "\n",
    "        idx = (z >= 0.0)\n",
    "        a[idx] = 1.0 / (1.0 + np.exp(-z[idx]))\n",
    "\n",
    "        idx = np.invert(idx) # Same as idx = (z < 0.0)\n",
    "        a[idx] = np.exp(z[idx]) / (1.0 + np.exp(z[idx]))\n",
    "\n",
    "        return a\n",
    "\n",
    "\n",
    "    # Linear Activation: Actually does nothing!\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # Perform an activation on last layer of ANFIS\n",
    "    def activation_layer(self, fifth_layer_out, problem_category):\n",
    "        # problem_category: C == Classification , R = Regression\n",
    "        if problem_category == \"C\":\n",
    "            sigmoid_out = self.sigmoid(fifth_layer_out)\n",
    "            idx = np.argmax(sigmoid_out.reshape(-1,1))\n",
    "            y_predict = np.zeros(self.network_IO_size[1])\n",
    "            y_predict[idx]=1.0\n",
    "            return y_predict\n",
    "        \n",
    "        elif problem_category == \"R\":\n",
    "            return self.linear(fifth_layer_out)\n",
    "#             return sigmoid(self.linear(fifth_layer_out))\n",
    "        \n",
    "      \n",
    "    # forwardpass for computing output for one input value\n",
    "    def forwardpass(self, input_data, problem_category):\n",
    "\n",
    "        first_layer_out = self.mf_generators(input_data=input_data)\n",
    "        second_layer_out = self.second_layer(first_layer_out)\n",
    "        third_layer_out = self.third_layer(second_layer_out)\n",
    "        fourth_layer_out = self.fourth_layer(input_data, third_layer_out)\n",
    "        fifth_layer_out = self.fifth_layer(fourth_layer_out)\n",
    "        final_output = self.activation_layer(fifth_layer_out, problem_category)\n",
    "        return final_output\n",
    "        \n",
    "        \n",
    "    # evaluate ANFIS with a dataset   \n",
    "    def eval_data(self, dataset, problem_category, log=False):   \n",
    "        y_true = dataset.values[:,-1]\n",
    "        x = dataset.values[:,:-1]\n",
    "        \n",
    "        if problem_category==\"C\":\n",
    "            y_true, _ = build_class_matrix(y_true)\n",
    "\n",
    "        y_pred = np.zeros_like(y_true)\n",
    "        \n",
    "        if self.p_q_r_values is None:\n",
    "            self.p_q_r_values = np.random.rand(self.network_IO_size[1], self.fuzzy_terms_per_feature, len(x[0])+1)\n",
    "            # these values shoud be learned and tuned....\n",
    "            \n",
    "        if self.mf_parameters is None:\n",
    "            if self.mf_mode == \"tri\":\n",
    "                mf_dim = 2\n",
    "            elif self.mf_mode == \"gbell\":\n",
    "                mf_dim = 3\n",
    "        \n",
    "            self.mf_parameters = np.random.rand(self.fuzzy_terms_per_feature, len(x[0]), mf_dim)\n",
    "            # these values shoud be learned and tuned....\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            y_pred[i] = self.forwardpass(input_data=x[i], problem_category=problem_category)\n",
    "        if problem_category==\"R\":\n",
    "            mse =  mean_squared_error(y_true, y_pred)\n",
    "            if log:\n",
    "                print(f\"MSE: {mse}\")\n",
    "            return [mse, y_pred]\n",
    "\n",
    "        elif problem_category==\"C\": \n",
    "            acc =  accuracy_score(y_true, y_pred)  \n",
    "            categorical_loss =  log_loss(y_true, y_pred)\n",
    "            if log:\n",
    "                print(f\"Accuracy: {acc} ,Categorical loss: {categorical_loss}\")\n",
    "            return [categorical_loss, acc, y_pred]\n",
    "\n",
    "\n",
    "# normalize a dataframe (Max-Min scaler)\n",
    "def dataframe_normalizer(df):\n",
    "    # copy the data\n",
    "    df_min_max_scaled = df.copy()\n",
    "    \n",
    "    # apply normalization techniques\n",
    "    for column in df_min_max_scaled.columns:\n",
    "        df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())    \n",
    "  \n",
    "    return df_min_max_scaled\n",
    "\n",
    "\n",
    "# calcultor network output size\n",
    "def network_out_size_calcultor(problem_category, df=None):\n",
    "    if problem_category==\"R\":\n",
    "        return 1\n",
    "    elif problem_category==\"C\": \n",
    "        y_u, idx = np.unique(df.values[:,-1], return_inverse=True)\n",
    "        return len(y_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031469b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "143ee370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_type(dataset):\n",
    "    if dataset[1] == \"R\":\n",
    "        return \"regreesion\"\n",
    "    elif dataset[1] == \"C\":\n",
    "        return \"classification\"\n",
    "    \n",
    "\n",
    "def build_network_with_sample_dataset(dataset, fuzzy_terms_per_feature, mf_mode=\"gbell\", split_ratio=0.2):\n",
    "    df = pd.read_csv(f\"./data/{dataset[0]}\", header = None)\n",
    "    task_type_short = dataset[1]\n",
    "    print(f\"This is a {task_type_short}({task_type(dataset)}) task\")\n",
    "    # we shoud re-scale if we want to see the real ranged prediction.\n",
    "    df_train, df_test = train_test_split(df, test_size=split_ratio, shuffle=True)\n",
    "    \n",
    "    df_train = dataframe_normalizer(df_train)\n",
    "    df_test = dataframe_normalizer(df_test)\n",
    "    \n",
    "    # df_train.describe()\n",
    "    out_size = network_out_size_calcultor(problem_category=dataset[1], df=df_train)\n",
    "    network=ANFIS(network_IO_size=(df_train.values[0].shape[0]-1,out_size), fuzzy_terms_per_feature=fuzzy_terms_per_feature, mf_mode=mf_mode)\n",
    "    print(\"Performance of network with initial random parameters: \")\n",
    "    print(\"Init train results:\")\n",
    "    init_train = network.eval_data(df_train, problem_category=dataset[1], log=True)\n",
    "    print(\"Init test results:\")\n",
    "    init_test = network.eval_data(df_test, problem_category=dataset[1], log=True)\n",
    "    return network, df_train, df_test, init_train, init_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7966784",
   "metadata": {},
   "source": [
    "# PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0f3cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_wrapper(network, mode, all_params_together=None):\n",
    "    p_q_r_values_shape = network.p_q_r_values.shape\n",
    "    mf_parameters_shape = network.mf_parameters.shape\n",
    "    p_q_r_count = np.prod(p_q_r_values_shape)\n",
    "    mf_count = np.prod(mf_parameters_shape)\n",
    "    all_parameters_count = p_q_r_count + mf_count\n",
    "    \n",
    "#     print(p_q_r_values_shape)\n",
    "#     print(mf_parameters_shape)\n",
    "#     print(p_q_r_count)\n",
    "#     print(mf_count)\n",
    "#     print(all_parameters_count)\n",
    "#     print(all_params_together)\n",
    "#     print(all_params_together.shape)\n",
    "#     print(p_q_r_flatten)\n",
    "#     print(mf_flatten)\n",
    "    \n",
    "    if mode==\"wrap\":\n",
    "        p_q_r_flatten = network.p_q_r_values.flatten()\n",
    "        mf_flatten = network.mf_parameters.flatten()\n",
    "        all_params_together = np.concatenate((p_q_r_flatten, mf_flatten), axis=0)\n",
    "        return all_params_together\n",
    "    \n",
    "    elif mode==\"unwrap\":\n",
    "        p_q_r = all_params_together[:p_q_r_count].reshape(p_q_r_values_shape)\n",
    "        mf = all_params_together[p_q_r_count:].reshape(mf_parameters_shape)\n",
    "        return p_q_r, mf\n",
    "    \n",
    "    elif mode==\"params_count\":\n",
    "        return all_parameters_count\n",
    "    \n",
    "    \n",
    "def suitable_form_of_network_as_function(params, mode=0, inference=False):\n",
    "    problem_category=dataset[1]\n",
    "    p_q_r, mf = parameters_wrapper(network, mode=\"unwrap\", all_params_together=params)\n",
    "    network.p_q_r_values = p_q_r\n",
    "    network.mf_parameters = mf\n",
    "    if mode==0:\n",
    "        df_to_run = df_train\n",
    "    elif mode==1:\n",
    "        df_to_run = df_test\n",
    "\n",
    "    out = network.eval_data(df_to_run, problem_category=dataset[1], log=False)\n",
    "        \n",
    "    if inference:\n",
    "        return out\n",
    "    else:\n",
    "        loss = out[0]\n",
    "        return loss\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Higher-level method to do forward_prop in the\n",
    "    whole swarm.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: numpy.ndarray of shape (n_particles, dimensions)\n",
    "        The swarm that will perform the search\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of shape (n_particles, )\n",
    "        The computed loss for each particle\n",
    "    \"\"\"\n",
    "    n_particles = x.shape[0]\n",
    "    j = [suitable_form_of_network_as_function(x[i]) for i in range(n_particles)]\n",
    "    return np.array(j)\n",
    "\n",
    "\n",
    "def predict(params, df):\n",
    "    \n",
    "    problem_category=dataset[1]\n",
    "    p_q_r, mf = parameters_wrapper(network, mode=\"unwrap\", all_params_together=params)\n",
    "    network.p_q_r_values = p_q_r\n",
    "    network.mf_parameters = mf\n",
    "    out = network.eval_data(df, problem_category=dataset[1], log=False)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "827b217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_params_together = parameters_wrapper(network, mode=\"wrap\")\n",
    "# np.arange(0, 1, 0.03)\n",
    "# suitable_form_of_network_as_function(params=all_params_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63170634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PSO_on_network(network, n_particles=1, iters=10):\n",
    "    dimensions_count = parameters_wrapper(network, mode=\"params_count\")\n",
    "    # print(dimensions_count)\n",
    "    \n",
    "    # Set-up hyperparameters\n",
    "    options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}\n",
    "    # Call instance of PSO\n",
    "    optimizer = ps.single.GlobalBestPSO(n_particles=n_particles, dimensions=dimensions_count, options=options)\n",
    "    # Perform optimization\n",
    "    cost, pos = optimizer.optimize(f, iters=iters)\n",
    "    return cost, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8922f",
   "metadata": {},
   "source": [
    "# Alltogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d4a6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reult_printer(pos, init_train, init_test, dataset=dataset):\n",
    "    init_loss_train = init_train[0]\n",
    "    init_loss_test = init_test[0]\n",
    "    problem_category=dataset[1]\n",
    "    inference = True\n",
    "    \n",
    "    # Check on tetrainst set\n",
    "    train_out = suitable_form_of_network_as_function(pos, mode=0, inference=inference)\n",
    "    train_loss = train_out[0]\n",
    "    print(f\"Train loss: {train_loss}, Init train loss: {init_loss_train}\")\n",
    "\n",
    "    # Check on test set\n",
    "    test_out = suitable_form_of_network_as_function(pos, mode=1, inference=inference)\n",
    "    test_loss = test_out[0]\n",
    "    print(f\"Test loss: {test_loss}, Init test loss: {init_loss_test}\")\n",
    "    \n",
    "    if problem_category==\"C\":\n",
    "        # Check on tetrainst set\n",
    "        train_acc = train_out[1]\n",
    "        print(f\"Train acc: {train_acc}, Init train acc: {init_train[1]}\")\n",
    "\n",
    "        # Check on test set\n",
    "        test_acc = test_out[1]\n",
    "        print(f\"Test acc: {test_acc}, Init test acc: {init_test[1]}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2401650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[\"stock_dataset.csv\", \"R\"]\n",
    "w=[\"wine_dataset.csv\", \"C\"]\n",
    "p=[\"pulsar_dataset.csv\", \"C\"]\n",
    "l=[\"plant_dataset.csv\", \"R\"]\n",
    "\n",
    "# Code works on all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b9bdbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a R(regreesion) task\n",
      "Performance of network with initial random parameters: \n",
      "Init train results:\n",
      "MSE: 1.3406646332954915\n",
      "Init test results:\n",
      "MSE: 1.4021133707936864\n"
     ]
    }
   ],
   "source": [
    "dataset = s\n",
    "\n",
    "network, df_train, df_test, init_train, init_test = build_network_with_sample_dataset(dataset=dataset, fuzzy_terms_per_feature=3, mf_mode=\"gbell\", split_ratio=0.2)\n",
    "# Don't change the names. These are global variable because of passing in a suitable form to PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0160bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 05:00:32,265 - pyswarms.single.global_best - INFO - Optimize for 10 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|████████████████████████████████████████████████████████████████████████████████████████████|10/10, best_cost=0.00703\n",
      "2022-01-12 05:03:10,657 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.007034484250286584, best pos: [-0.18523599  0.39525363  0.36044681  0.64968409  0.08335652 -0.30749874\n",
      "  0.04620737  0.12230104  0.65568481  0.17458391 -0.12110144  0.18570054\n",
      "  0.37792509  0.07633825  0.21825258 -0.34777294  0.66558111  1.20971793\n",
      "  0.43717829  0.55777829  1.10947873  0.11336077  0.37063121 -0.1081405\n",
      "  0.21858808  0.0251866   0.59694481  0.32052406  1.25376305  0.20198665\n",
      "  0.47344497  0.44563173  0.36493497  0.45099722  0.02210589  0.08023556\n",
      "  0.44114211 -0.03350152  0.42665905  0.785043    0.56248504  0.33389951\n",
      "  0.83495996  0.14155413  0.49893157  0.64485829  1.04824707 -0.00953864\n",
      " -0.22497878  0.26148151  0.87003354]\n"
     ]
    }
   ],
   "source": [
    "cost, pos = run_PSO_on_network(network, n_particles=100, iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df5bf822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.007034484250286584, Init train loss: 1.3406646332954915\n",
      "Test loss: 0.010499653358208613, Init test loss: 1.4021133707936864\n"
     ]
    }
   ],
   "source": [
    "reult_printer(pos, init_train, init_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ab6a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check predictions of network for a custom dataframe\n",
    "out_ = predict(pos, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7666e18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010499653358208613,\n",
       " array([0.42304904, 0.59229546, 0.52754335, 0.74771894, 0.62050896,\n",
       "        0.49851703, 0.45749254, 0.48186433, 0.44228814, 0.7730616 ,\n",
       "        0.38215479, 0.47337344, 0.53340977, 0.51905878, 0.5410792 ,\n",
       "        0.46173247, 0.5084291 , 0.44848823, 0.48048714, 0.51461007,\n",
       "        0.59347607, 0.6148059 , 0.50267023, 0.63318441, 0.50043389,\n",
       "        0.5687001 , 0.48936193, 0.49871466, 0.45531364, 0.63142014,\n",
       "        0.38588105, 0.65994067, 0.65132919, 0.66243556, 0.49879881,\n",
       "        0.53410082, 0.51392695, 0.52904737, 0.49391241, 0.4742666 ,\n",
       "        0.52097718, 0.57277136, 0.50959127, 0.62221771, 0.301177  ,\n",
       "        0.43437074, 0.52768495, 0.52509291, 0.52720698, 0.46181462,\n",
       "        0.48442287, 0.52579817, 0.58633026, 0.52430204, 0.60542231,\n",
       "        0.49749015, 0.51041932, 0.56116658, 0.51166366, 0.61310072,\n",
       "        0.66601084, 0.41677215, 0.51511935, 0.56077655, 0.52296668,\n",
       "        0.53948317, 0.57737232, 0.68821564, 0.49502899, 0.52388567,\n",
       "        0.54153319, 0.5682307 , 0.54798865, 0.49789428, 0.52396379,\n",
       "        0.47908666, 0.63591671, 0.52943408, 0.42643749, 0.65306511,\n",
       "        0.57927904, 0.19329491, 0.87419452, 0.42080086, 0.59952141,\n",
       "        0.72520632, 0.43122471, 0.60112005, 0.47541679, 0.28915664,\n",
       "        0.64968649, 0.70759891, 0.50240225, 0.28078277, 0.44365111,\n",
       "        0.4854008 , 0.37655054, 0.53337658, 0.71038402, 0.51869947,\n",
       "        0.58576161, 0.5921483 , 0.62835864, 0.74646852, 0.63247152,\n",
       "        0.50248976, 0.31959806, 0.57694265])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_\n",
    "# [categorical_loss, acc, y_pred] for Classification task\n",
    "# [mse, y_pred] for Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46f180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
